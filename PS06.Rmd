---
title: "STAT/MATH 495: Problem Set 06"
author: "Syed Abbas Shah"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(mosaic)
library(broom)
library(knitr)
library(gridExtra)
```





# Collaboration

Please indicate who you collaborated with on this assignment: 

No one



# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation


```{r, echo=FALSE, message=FALSE, warning=FALSE}
s<-generate_sample(f,n,sigma)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
baseplot <- ggplot(data=NULL) +
  geom_vline(xintercept = x0, linetype="dashed") +
  geom_point(data=test_set, aes(x=.95,y=.9025), col="brown", size=4.5) +
  coord_cartesian(xlim=c(0, 1), ylim=c(-0.75, 1.5))

plot_df_2 <- baseplot +
  labs(title="Degrees of freedom = 2")
plot_df_99 <- baseplot +
  labs(title="Degrees of freedom = 99")
```


Let's try our two spline models on 40 samples randomly generated from our function. This is so we can visualize what we're trying to do here.

```{r, message=FALSE, warning=FALSE}
for(i in 1:40) {
  sampled_points <- generate_sample(f, n, sigma)
  # Fit splines with df=2 and add to plot
  fitted_df_2 <- smooth.spline(x=sampled_points$x, y=sampled_points$y, df=2) %>%
    augment()
  plot_df_2 <- plot_df_2 +
    geom_point(data=fitted_df_2, aes(x=x, y=y), alpha=0.1) +
    geom_line(data=fitted_df_2, aes(x=x, y=.fitted), col="blue", alpha=.8, size=1.25)
  # Fit splines with df=99 and add to plot
  fitted_df_99 <- smooth.spline(x=sampled_points$x, y=sampled_points$y, df=99) %>%
    augment()
  plot_df_99 <- plot_df_99 +
    geom_point(data=fitted_df_99, aes(x=x, y=y), alpha=0.1) +
    geom_line(data=fitted_df_99, aes(x=x, y=.fitted), col="blue", alpha=0.8, size=1.25)
  
}
plot_df_2 <- plot_df_2 +
  geom_point(data=data_frame(x=x0, y=0.9025), aes(x=x,y=y), col="red", size=4.5)
plot_df_99 <- plot_df_99 +
  geom_point(data=data_frame(x=x0, y=0.9025), aes(x=x,y=y), col="red", size=4.5)
grid.arrange(plot_df_2,plot_df_99)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
baseplot <- ggplot(data=NULL) +
  geom_vline(xintercept = x0, linetype="dashed") +
  geom_point(data=test_set, aes(x=.95,y=.9025), col="brown", size=4.5) +
  coord_cartesian(xlim=c(0, 1), ylim=c(-0.75, 1.5))

plot_df_2 <- baseplot +
  labs(title="Degrees of freedom = 2")
plot_df_99 <- baseplot +
  labs(title="Degrees of freedom = 99")
```



# Tables


Let's simulate 2000 points and compute values for the error terms


```{r, message=FALSE, warning=FALSE}
x0<-0.95;y0 <- .95^2 #actual y0
estimatedy2<-0;estimatedy99<-0
set.seed(50)
for(i in 1:2000){  
sampled_points <- generate_sample(f, n, sigma)
model2 <- smooth.spline(x=sampled_points$x, y=sampled_points$y, df=2)
estimatedy2[i]<- predict(model2,x0)$y
model99 <- smooth.spline(x=sampled_points$x, y=sampled_points$y, df=99)
estimatedy99[i]<- predict(model99,x0)$y
}  

```

Now, we can do all the necessary computations.
```{r, message=FALSE, warning=FALSE}
set.seed(100)
observedy = f(x0) + rnorm(2000, 0, sigma) #generating 'true values'
data2 <- data.frame(observedy)
data99<- data.frame(observedy)
data2$estimatedy2 <- estimatedy2
data99$estimatedy99 <- estimatedy99
data2$residmodel2 <- (observedy-estimatedy2)
data99$residmodel99<- (observedy-estimatedy99)

errordecomposition2 <- data2 %>% 
  summarize(Name = "Spline df 2",Variance = var(estimatedy2), `Bias Squared` =(mean(estimatedy2)-y0)^2, MSE = mean(residmodel2^2), `Irreducible Error` = sigma^2) 
errordecomposition2$Sum = errordecomposition2$`Bias Squared`+errordecomposition2$Variance + errordecomposition2$`Irreducible Error`

errordecomposition99 <- data99 %>%
  summarize(Name = "Spline df 99",Variance = var(estimatedy99), `Bias Squared` =(mean(estimatedy99)-y0)^2, MSE = mean(residmodel99^2), `Irreducible Error` = sigma^2) 
errordecomposition99$Sum = errordecomposition99$`Bias Squared`+errordecomposition99$Variance + errordecomposition99$`Irreducible Error`

Error <- rbind(errordecomposition2,errordecomposition99)
```

Now, I'm going to print the decomposed error, and contrast the components between the linear (df=2) and the extremely flexible (df=99) model.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(Error, digits=4, format="html", caption = "Error Comparison")
```

This is unsurprising. The linear model has lower variance than the df 99 one, and the latter has lower bias squared. The MSE in the linear model is a bit less than the df 99 one, because the latter is very overfit.

# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.

2. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.

3. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. The plots should show that low flexibility models should be have lower variance (fewer 'wiggles') as opposed to higher flexibility models. That is, the linear model should have lower variance and higher bias than the df=99 model, for the latter is much more flexible and is attuned to the idiosyncrasies of the training sample. 
Moreover, the overall error can never be lower than the irreducible error. If that appears to be the case, something has gone wrong.

2. We would have to look at the MSE in estimates for all values by computing the difference between the estimates and the true values. That means we would need to repeat what I did above for all values of x that interest us. I.e., instead of having a test set with just 0.95, we would need to simulate function values for the entire domain of x, predict y_hat for the domain, and then compute the respective MSE values.

3. If the goal were simply to predict the point of interest, I would probably not pick either of these models. That said, between both, I would opt for the Linear model. This is because the variance is substantially lower, and because the df=99 model is extremely overfit, leading to poorer predictions. Thus, even if the Linear model has higher bias, it would give better predictions than a highly overfit model.



